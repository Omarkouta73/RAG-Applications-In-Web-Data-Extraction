{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "collapsed_sections": [
        "H8UGSkD6vmES"
      ],
      "provenance": [],
      "gpuType": "T4",
      "name": "smallModelsRAG",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omarkouta73/RAG-Applications-In-Web-Data-Extraction/blob/main/Text/smallModelsRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip install faiss-cpu\n",
        "# !pip install selenium\n",
        "# !pip install google-generativeai\n",
        "# !pip install -qU langchain-text-splitters\n",
        "# !pip install langchain_experimental\n",
        "!pip install \"unstructured[all-docs]\" unstructured"
      ],
      "metadata": {
        "id": "s5hI08vkEm2k",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:12:22.253191Z",
          "iopub.execute_input": "2025-03-17T19:12:22.25356Z",
          "iopub.status.idle": "2025-03-17T19:12:38.554502Z",
          "shell.execute_reply.started": "2025-03-17T19:12:22.253532Z",
          "shell.execute_reply": "2025-03-17T19:12:38.553561Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import google.generativeai as genai\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_text_splitters import HTMLSemanticPreservingSplitter\n",
        "from unstructured.partition.html import partition_html\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import random\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pwA6g6YnEbvg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:12:50.626543Z",
          "iopub.execute_input": "2025-03-17T19:12:50.6269Z",
          "iopub.status.idle": "2025-03-17T19:12:52.900611Z",
          "shell.execute_reply.started": "2025-03-17T19:12:50.626868Z",
          "shell.execute_reply": "2025-03-17T19:12:52.899777Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Fetch HTML content\n",
        "def get_cleaned_text(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Remove the specific div with id=\"customer-reviews_feature_div\"\n",
        "    customer_reviews_div = soup.find('div', id='customer-reviews_feature_div')\n",
        "    if customer_reviews_div and hasattr(customer_reviews_div, 'decompose'):\n",
        "        customer_reviews_div.decompose()\n",
        "\n",
        "    # Remove unnecessary tags (scripts, styles, iframes, etc.)\n",
        "    for tag in soup.find_all(['script', 'style', 'iframe', 'noscript', 'footer', 'header', 'a']):\n",
        "        if tag and hasattr(tag, 'decompose'):\n",
        "            tag.decompose()\n",
        "\n",
        "    # Remove tags with ad-related keywords\n",
        "    ad_keywords = ['ad', 'banner', 'promo', 'footer', 'sponsor', 'select', 'button']\n",
        "    for tag in soup.find_all(ad_keywords):\n",
        "        if tag and hasattr(tag, 'decompose'):\n",
        "            tag.decompose()\n",
        "\n",
        "    # Remove empty tags\n",
        "    for tag in soup.find_all():\n",
        "        if tag and not tag.get_text(strip=True):\n",
        "            if hasattr(tag, 'decompose'):\n",
        "                tag.decompose()\n",
        "\n",
        "    return soup\n",
        "\n",
        "def fetch_html(url):\n",
        "    # List of user agents\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'\n",
        "    ]\n",
        "\n",
        "    # Chrome options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")  # Run headless\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(f\"user-agent={random.choice(user_agents)}\")\n",
        "\n",
        "    # Set location to Egypt\n",
        "    chrome_options.add_argument(\"--lang=ar-EG\")\n",
        "    chrome_options.add_experimental_option(\"prefs\", {\n",
        "        \"intl.accept_languages\": \"ar-EG,ar\",\n",
        "        \"profile.default_content_setting_values.geolocation\": 1\n",
        "    })\n",
        "\n",
        "    # Add geolocation for Cairo, Egypt\n",
        "    chrome_options.add_argument(\"--enable-geolocation\")\n",
        "    chrome_options.add_argument(\"--use-fake-ui-for-media-stream\")\n",
        "    chrome_options.add_argument(\"--use-fake-device-for-media-stream\")\n",
        "    chrome_options.add_experimental_option(\"prefs\", {\n",
        "        \"profile.default_content_settings.geolocation\": 1,\n",
        "        \"profile.managed_default_content_settings.geolocation\": 1,\n",
        "        \"profile.default_content_setting_values.geolocation\": 1\n",
        "    })\n",
        "\n",
        "    service = Service()  # Windows\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "    driver.execute_cdp_cmd(\"Emulation.setGeolocationOverride\", {\n",
        "        \"latitude\": 30.0444,\n",
        "        \"longitude\": 31.2357,\n",
        "        \"accuracy\": 100\n",
        "    })\n",
        "\n",
        "    time.sleep(random.uniform(5, 10))\n",
        "\n",
        "    driver.get(url)\n",
        "\n",
        "    return get_cleaned_text(driver.page_source)\n",
        "\n",
        "def read_html(dir):\n",
        "  with open(dir, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()"
      ],
      "metadata": {
        "id": "TnDT7lVcN0gT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:09:17.683035Z",
          "iopub.execute_input": "2025-03-17T19:09:17.683392Z",
          "iopub.status.idle": "2025-03-17T19:09:17.692102Z",
          "shell.execute_reply.started": "2025-03-17T19:09:17.68335Z",
          "shell.execute_reply": "2025-03-17T19:09:17.691238Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def read_md(dir):\n",
        "  try:\n",
        "      with open(dir, \"r\", encoding=\"utf-8\") as file:\n",
        "          content = file.read()\n",
        "      return content\n",
        "  except FileNotFoundError:\n",
        "      print(f\"Error: The file '{dir}' was not found.\")\n",
        "      return None\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred while reading the file: {e}\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "GdG2HRJBeJ4h",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:09:17.694418Z",
          "iopub.execute_input": "2025-03-17T19:09:17.694693Z",
          "iopub.status.idle": "2025-03-17T19:09:17.725331Z",
          "shell.execute_reply.started": "2025-03-17T19:09:17.694673Z",
          "shell.execute_reply": "2025-03-17T19:09:17.724462Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def langchain_SC(text, number_of_chunks=16, breakpoint_threshold_type=\"standard_deviation\", model=\"all-MiniLM-L6-v2\"):\n",
        "  embedding_model = HuggingFaceEmbeddings(model_name=model)\n",
        "  splitter = SemanticChunker(embedding_model, number_of_chunks=16, breakpoint_threshold_type=\"standard_deviation\")\n",
        "  chunks = splitter.create_documents([text])\n",
        "  return chunks\n",
        "\n",
        "\n",
        "def langchain_HSC(text):\n",
        "  headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\n",
        "  splitter = HTMLSemanticPreservingSplitter(\n",
        "      headers_to_split_on=headers_to_split_on,\n",
        "      max_chunk_size=500,\n",
        "      elements_to_preserve=[\"table\", \"ul\", \"li\", \"ol\", \"p\"],\n",
        "      chunk_overlap=0,\n",
        "      denylist_tags=[\"script\", \"style\", \"head\"],\n",
        "      normalize_text=True,\n",
        "      preserve_images=True,\n",
        "      preserve_links=True,\n",
        "      stopword_removal=False,\n",
        "  )\n",
        "  chunks = splitter.split_text(text)\n",
        "  return chunks\n",
        "\n",
        "nltk.download('stopwords')\n",
        "# Define a set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stop_words(text):\n",
        "  # Split the text into words, filter out stopwords, and join them back together.\n",
        "  return ' '.join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "def unstructured_ch(text, chunking_strategy=\"by_title\"):\n",
        "    # Partition the HTML/text into chunks\n",
        "    chunks = partition_html(text=text, chunking_strategy=chunking_strategy)\n",
        "    # For each chunk, lower-case the text and remove stop words\n",
        "    return [remove_stop_words(chunk.text.lower()) for chunk in chunks]"
      ],
      "metadata": {
        "id": "3vGeovHxE037",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:12:58.620819Z",
          "iopub.execute_input": "2025-03-17T19:12:58.62116Z",
          "iopub.status.idle": "2025-03-17T19:12:58.674824Z",
          "shell.execute_reply.started": "2025-03-17T19:12:58.621131Z",
          "shell.execute_reply": "2025-03-17T19:12:58.673927Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Text Embedding\n",
        "def embed_text(text, model):\n",
        "    # Move model to GPU\n",
        "    #model.to(device)\n",
        "\n",
        "    # Get token embeddings\n",
        "    token_embeddings = model(text, return_tensors=\"pt\")[0].to(device)  # Move embeddings to GPU\n",
        "    # Average the token embeddings to get a single vector\n",
        "    chunk_embedding = token_embeddings.mean(dim=0).detach().cpu().numpy()  # Move back to CPU for FAISS\n",
        "    return chunk_embedding"
      ],
      "metadata": {
        "id": "olHSZGwWHxcj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:09:17.745684Z",
          "iopub.execute_input": "2025-03-17T19:09:17.74595Z",
          "iopub.status.idle": "2025-03-17T19:09:17.766104Z",
          "shell.execute_reply.started": "2025-03-17T19:09:17.745926Z",
          "shell.execute_reply": "2025-03-17T19:09:17.765265Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Query Embedding and Similarity Search\n",
        "def search_similar_chunks(query, chunks, embeddings, model, top_k=3):\n",
        "    query_embedding = embed_text(query, model)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "    return [chunks[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "MPLzJ2XFH1i7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:09:17.766929Z",
          "iopub.execute_input": "2025-03-17T19:09:17.767187Z",
          "iopub.status.idle": "2025-03-17T19:09:17.78911Z",
          "shell.execute_reply.started": "2025-03-17T19:09:17.767167Z",
          "shell.execute_reply": "2025-03-17T19:09:17.788362Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: LLM Processing with Gemini\n",
        "def extract_info_with_llm(chunks, prompt):\n",
        "    # Combine chunks into a single string\n",
        "    combined_chunks = \" \".join(chunks)\n",
        "\n",
        "    # Initialize Gemini\n",
        "    genai.configure(api_key=\"AIzaSyBmJlrhubrcT6vGf7APloGaxvEi-xKTIk4\")  # Replace with your Gemini API key\n",
        "\n",
        "    # Use the Gemini model\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash')  # Use the Gemini Pro model\n",
        "\n",
        "    # Generate a response\n",
        "    response = model.generate_content(f\"\"\"using these information: {combined_chunks} make your best to {prompt}\n",
        "                                      , and provide it in ```json ``` format.\n",
        "                                      give me 'NAN' if there is no direct answer in the provided information.\"\"\",\n",
        "                                      generation_config={\"max_output_tokens\": 500})\n",
        "\n",
        "    # Return the generated text\n",
        "    return response.text\n",
        "\n",
        "def extract_info_with_qwen(chunks, prompt):\n",
        "    # Combine chunks into a single string\n",
        "    combined_chunks = \" \".join(chunks)\n",
        "    # Load Qwen model and tokenizer\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "    # Construct the input prompt\n",
        "    input_text = f\"\"\"using these information: {combined_chunks} make your best to {prompt}\n",
        "                    , and provide it in ```json ``` format.\n",
        "                    give me 'NAN' if there is no direct answer in the provided information.\"\"\"\n",
        "    # Tokenize input and generate response\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=100)\n",
        "    # Decode response\n",
        "    full_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Find where the actual generated response begins\n",
        "    # This assumes the model's response starts after the input prompt\n",
        "    if input_text in full_response:\n",
        "        # Extract only the generated part\n",
        "        actual_response = full_response[len(input_text):].strip()\n",
        "        return actual_response\n",
        "    else:\n",
        "        # If we can't find a clean separation, return the full response\n",
        "        # but log a warning\n",
        "        print(\"Warning: Could not separate input from output. Returning full response.\")\n",
        "        return full_response"
      ],
      "metadata": {
        "id": "z9_df-QFH4_9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:29:36.825367Z",
          "iopub.execute_input": "2025-03-17T19:29:36.825682Z",
          "iopub.status.idle": "2025-03-17T19:29:36.831918Z",
          "shell.execute_reply.started": "2025-03-17T19:29:36.825657Z",
          "shell.execute_reply": "2025-03-17T19:29:36.831104Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "url = \"https://www.amazon.com/A315-24P-R7VH-Display-Quad-Core-Processor-Graphics/dp/B0BS4BP8FB/ref=sr_1_3?sr=8-3\"\n",
        "html = fetch_html(url)"
      ],
      "metadata": {
        "id": "8SX0Cd3HOSQL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:09:17.813167Z",
          "iopub.execute_input": "2025-03-17T19:09:17.813416Z",
          "iopub.status.idle": "2025-03-17T19:09:39.679926Z",
          "shell.execute_reply.started": "2025-03-17T19:09:17.813395Z",
          "shell.execute_reply": "2025-03-17T19:09:39.678924Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#html = read_html(\"/content/page.html\")"
      ],
      "metadata": {
        "id": "Q_nKJprNOUUY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:10:20.449893Z",
          "iopub.execute_input": "2025-03-17T19:10:20.450216Z",
          "iopub.status.idle": "2025-03-17T19:10:20.45461Z",
          "shell.execute_reply.started": "2025-03-17T19:10:20.450192Z",
          "shell.execute_reply": "2025-03-17T19:10:20.453233Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "techniques = {\n",
        "    \"chunks_SC\": langchain_SC(str(html)),\n",
        "    \"chunks_HSC\": langchain_HSC(str(html)),\n",
        "    \"Unstructured\": unstructured_ch(str(html))\n",
        "}"
      ],
      "metadata": {
        "id": "CfbKqUTIc6-Q",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:13:47.682688Z",
          "iopub.execute_input": "2025-03-17T19:13:47.683021Z",
          "iopub.status.idle": "2025-03-17T19:13:56.904658Z",
          "shell.execute_reply.started": "2025-03-17T19:13:47.682993Z",
          "shell.execute_reply": "2025-03-17T19:13:56.903599Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Embed chunks\n",
        "model1 = pipeline('feature-extraction', model='distilbert-base-uncased', padding=True, truncation=True, add_special_tokens = True)\n",
        "\n",
        "# Option 1: Better pre-trained model\n",
        "model2 = pipeline('feature-extraction',\n",
        "                 model='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                 padding=True,\n",
        "                 truncation=True,\n",
        "                 add_special_tokens=True)\n",
        "\n",
        "# # Option 2: Another good alternative\n",
        "# model3 = pipeline('feature-extraction',\n",
        "#                  model='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "#                  padding=True,\n",
        "#                  truncation=True,\n",
        "#                  add_special_tokens=True)"
      ],
      "metadata": {
        "id": "rJBIvyJ-ICYD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:14:26.769389Z",
          "iopub.execute_input": "2025-03-17T19:14:26.77016Z",
          "iopub.status.idle": "2025-03-17T19:14:40.332197Z",
          "shell.execute_reply.started": "2025-03-17T19:14:26.770129Z",
          "shell.execute_reply": "2025-03-17T19:14:40.331511Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual"
      ],
      "metadata": {
        "id": "H8UGSkD6vmES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chunks = techniques['Text Splitter']; chunks #ch_con = [chunk.page_content for chunk in chunks];"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mvwzyGdSkc8u",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddings = np.array([embed_text(chunk, model) for chunk in chunks])"
      ],
      "metadata": {
        "id": "9g_U50lAI9EA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Query\n",
        "#query = \"Extract product price\"\n",
        "#similar_chunks = search_similar_chunks(query, chunks, embeddings, top_k=3)"
      ],
      "metadata": {
        "id": "CJd9HwDVIOcP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#similar_chunks"
      ],
      "metadata": {
        "id": "wlFCXJYJRFLV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatically"
      ],
      "metadata": {
        "id": "rrMYY2DRvq5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt(query, model, k=3):\n",
        "  prompt = query\n",
        "  # Measure each chunking technique\n",
        "  for key, chunks in techniques.items():\n",
        "      start_time = time.time()\n",
        "      if type(chunks[0]) != str:\n",
        "          chunks = [chunk.page_content for chunk in chunks]\n",
        "      print(key,\":\")\n",
        "      #print(\"Chunks: \", chunks)\n",
        "\n",
        "      embeddings = np.array([embed_text(chunk, model) for chunk in chunks])\n",
        "      print(\"Embeddings Shape: \", embeddings.shape)\n",
        "\n",
        "      similar_chunks = search_similar_chunks(query, chunks, embeddings, model, top_k=k)\n",
        "      #print(\"Similar Chunks: \", similar_chunks)\n",
        "      # Model: Qwen\n",
        "      result = extract_info_with_qwen(similar_chunks, prompt)\n",
        "      execution_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
        "\n",
        "      print(\"Response:\", result)\n",
        "      print(f\"Time: {execution_time:.2f} ms\")\n",
        "      print(\"==============================================================================\")"
      ],
      "metadata": {
        "id": "LGd9P0hsL6WQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:32:23.600466Z",
          "iopub.execute_input": "2025-03-17T19:32:23.600761Z",
          "iopub.status.idle": "2025-03-17T19:32:23.606797Z",
          "shell.execute_reply.started": "2025-03-17T19:32:23.60074Z",
          "shell.execute_reply": "2025-03-17T19:32:23.605928Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## distilbert-base-uncased"
      ],
      "metadata": {
        "id": "SFfWMrlLMzJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt(\"Extract the amount of RAM memory installed\", model1, k=3)"
      ],
      "metadata": {
        "id": "yFLe61Desu4W",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:32:25.919925Z",
          "iopub.execute_input": "2025-03-17T19:32:25.920222Z",
          "iopub.status.idle": "2025-03-17T19:35:02.258637Z",
          "shell.execute_reply.started": "2025-03-17T19:32:25.920198Z",
          "shell.execute_reply": "2025-03-17T19:35:02.257759Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt(\"Extract the laptop price\", model1, k=3)"
      ],
      "metadata": {
        "id": "wZZ_t-MfH5BZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:35:02.261045Z",
          "iopub.execute_input": "2025-03-17T19:35:02.261272Z",
          "iopub.status.idle": "2025-03-17T19:37:54.696596Z",
          "shell.execute_reply.started": "2025-03-17T19:35:02.261253Z",
          "shell.execute_reply": "2025-03-17T19:37:54.695791Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt(\"Does the laptop has front-facing camera ?\", model1, k=3)"
      ],
      "metadata": {
        "id": "cmso6bImBiGA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:37:54.698626Z",
          "iopub.execute_input": "2025-03-17T19:37:54.698858Z",
          "iopub.status.idle": "2025-03-17T19:39:42.25626Z",
          "shell.execute_reply.started": "2025-03-17T19:37:54.698838Z",
          "shell.execute_reply": "2025-03-17T19:39:42.255352Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## all-MiniLM-L6-v2"
      ],
      "metadata": {
        "id": "ANkW8mjrOfVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt(\"Extract the amount of RAM memory installed\", model2, k=3)"
      ],
      "metadata": {
        "id": "P3ET8b_FD-Dv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:39:42.258465Z",
          "iopub.execute_input": "2025-03-17T19:39:42.258724Z",
          "iopub.status.idle": "2025-03-17T19:41:40.473194Z",
          "shell.execute_reply.started": "2025-03-17T19:39:42.258701Z",
          "shell.execute_reply": "2025-03-17T19:41:40.472368Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt(\"Extract the laptop price\", model2, k=3)"
      ],
      "metadata": {
        "id": "IrCZq53yOg5G",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:41:40.474774Z",
          "iopub.execute_input": "2025-03-17T19:41:40.475004Z",
          "iopub.status.idle": "2025-03-17T19:42:19.608158Z",
          "shell.execute_reply.started": "2025-03-17T19:41:40.474984Z",
          "shell.execute_reply": "2025-03-17T19:42:19.607401Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt(\"Does the laptop has front camera ?\", model2, k=3)"
      ],
      "metadata": {
        "id": "qHY_2tdEOhZT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-17T19:42:19.609375Z",
          "iopub.execute_input": "2025-03-17T19:42:19.609611Z",
          "iopub.status.idle": "2025-03-17T19:45:30.402001Z",
          "shell.execute_reply.started": "2025-03-17T19:42:19.609592Z",
          "shell.execute_reply": "2025-03-17T19:45:30.401101Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "quDQh50yAOU1"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}